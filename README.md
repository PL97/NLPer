


## Questions

### About embeding layers
- [ ] how to choose the number of tokens?
- [x] Why embeadding layer?
```
Embedding layer enables us to convert each word into a fixed length vector of defined size. The resultant vector is a dense one with having real values instead of just 0’s and 1’s. The fixed length of word vectors helps us to represent words in a better way along with reduced dimensions.
```


### Readings

#### BERT
<a>https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial</a>